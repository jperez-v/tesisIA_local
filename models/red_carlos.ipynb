{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "M8xGkLXM6XhM",
        "outputId": "c5924fdc-6239-4dc9-ee03-de3ec645f64b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "attempted relative import with no known parent package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c5074fb38043>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# models/amc_model.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from models.base_model import BaseTFModel\n",
        "\n",
        "class NN(BaseTFModel):\n",
        "\n",
        "    def build_model(self):\n",
        "        # --- Squeeze-and-Excitation block ---\n",
        "        def squeeze_excite_block(input_tensor, ratio=16):\n",
        "            filters = input_tensor.shape[-1]\n",
        "            se = layers.GlobalAveragePooling1D()(input_tensor)\n",
        "            se = layers.Dense(filters // ratio, activation='relu')(se)\n",
        "            se = layers.Dense(filters, activation='sigmoid')(se)\n",
        "            se = layers.Reshape((1, filters))(se)\n",
        "            return layers.Multiply()([input_tensor, se])\n",
        "\n",
        "        # --- Inception block con residual y SE ---\n",
        "        def inception_res_block(input_tensor, filters, l2_reg=1e-4):\n",
        "            tower1 = layers.Conv1D(filters, 4, padding='same', activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(l2_reg))(input_tensor)\n",
        "            tower2 = layers.Conv1D(filters, 6, padding='same', activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(l2_reg))(input_tensor)\n",
        "            tower3 = layers.Conv1D(filters, 8, padding='same', activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(l2_reg))(input_tensor)\n",
        "\n",
        "            concat = layers.Concatenate()([tower1, tower2, tower3])\n",
        "            concat = layers.BatchNormalization()(concat)\n",
        "            se = squeeze_excite_block(concat)\n",
        "\n",
        "            # Ajuste de dimensiones si es necesario para residual\n",
        "            if input_tensor.shape[-1] != se.shape[-1]:\n",
        "                input_tensor = layers.Conv1D(se.shape[-1], 1, padding='same')(input_tensor)\n",
        "\n",
        "            out = layers.Add()([input_tensor, se])\n",
        "            out = layers.Activation('relu')(out)\n",
        "            return out\n",
        "\n",
        "        # --- Multi-head Attention block ---\n",
        "        def attention_block(input_tensor, num_heads=2, key_dim = 16):\n",
        "            norm = layers.LayerNormalization()(input_tensor)\n",
        "            attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(norm, norm)\n",
        "            attn_output = layers.Dropout(0.1)(attn_output)\n",
        "            out = layers.Add()([input_tensor, attn_output])\n",
        "            return out\n",
        "\n",
        "        # 1) Parámetros del modelo\n",
        "        mp = self.model_params\n",
        "        seq_len = int(mp.get('seq_len', 4096))\n",
        "        n_classes = int(mp.get('output_size', mp.get('n_classes', 7)))\n",
        "        filtros = int(mp.get('filters', 32))\n",
        "        num_heads = int(mp.get('num_heads', 2))\n",
        "        key_dim = int(mp.get('key_dim', 16))\n",
        "        regularizador = float(mp.get('regularizer', 2e-4))\n",
        "        densa = int(mp.get('dense', 32))\n",
        "        dropout = float(mp.get('dropout', 0.3))\n",
        "        pooling = int(mp.get('pooling', 2))\n",
        "\n",
        "\n",
        "\n",
        "        # 2) Entrada\n",
        "        inp = layers.Input(shape=(seq_len, 2), name='IQ_input')\n",
        "\n",
        "        # 3) Bloques iniciales\n",
        "        x = layers.Conv1D(filtros, kernel_size=8, strides=2, padding='same', activation='relu')(inp)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling1D(pool_size=pooling)(x)\n",
        "\n",
        "        # 4) Bloques Inception + SE + Residual\n",
        "        x = inception_res_block(x, filters=filtros, l2_reg = regularizador)\n",
        "        x = inception_res_block(x, filters=filtros, l2_reg = regularizador)\n",
        "\n",
        "        # 5) Bloque de Atención\n",
        "        x = attention_block(x, num_heads=num_heads)\n",
        "\n",
        "        # 6) Clasificación\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "        x = layers.Dense(densa, activation='relu', kernel_regularizer=regularizers.l2(regularizador))(x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "        outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "        # 7) Modelo final\n",
        "        model = models.Model(inputs=inp, outputs=outputs)\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHwDpeVo7JE4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}