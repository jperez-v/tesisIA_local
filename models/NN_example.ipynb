{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 2409,
     "status": "error",
     "timestamp": 1746557230947,
     "user": {
      "displayName": "JUAN PABLO PEREZ VARGAS",
      "userId": "16083669280358186612"
     },
     "user_tz": 300
    },
    "id": "M8xGkLXM6XhM",
    "outputId": "c5924fdc-6239-4dc9-ee03-de3ec645f64b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models.base_model import BaseTFModel\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class NN(BaseTFModel):\n",
    "    def build_model(self):\n",
    "        # 1) Coge solo los parámetros de la arquitectura\n",
    "        mp = self.model_params\n",
    "\n",
    "        seq_len   = int(mp.get('seq_len',    4096))\n",
    "        n_classes = int(mp.get('output_size', mp.get('n_classes')))  # o sacar de mp\n",
    "\n",
    "        # 2) Define tu Input con seq_len\n",
    "        inp = layers.Input(shape=(seq_len, 2), name='IQ_input')\n",
    "\n",
    "        # 3) Bloque Inception-1D\n",
    "        def inception_block(x, f):\n",
    "            c1 = layers.Conv1D(f, 3, padding='same', activation='relu')(x)\n",
    "            c2 = layers.Conv1D(f, 5, padding='same', activation='relu')(x)\n",
    "            c3 = layers.Conv1D(f, 7, padding='same', activation='relu')(x)\n",
    "            return layers.Concatenate()([c1, c2, c3])\n",
    "\n",
    "        x = inception_block(inp, mp.get('inception_filters', 32))\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(mp.get('pool_size', 4))(x)\n",
    "\n",
    "        # 4) Bloques residuales\n",
    "        def res_block(x, f):\n",
    "            y = layers.Conv1D(f, 3, padding='same', activation='relu')(x)\n",
    "            y = layers.BatchNormalization()(y)\n",
    "            y = layers.Conv1D(f, 3, padding='same')(y)\n",
    "            y = layers.BatchNormalization()(y)\n",
    "            if x.shape[-1] != f:\n",
    "                x = layers.Conv1D(f, 1, padding='same')(x)\n",
    "            return layers.Activation('relu')(layers.Add()([x, y]))\n",
    "\n",
    "        for f in mp.get('res_filters', [64, 128]):\n",
    "            x = res_block(x, f)\n",
    "            x = layers.MaxPooling1D(mp.get('pool_size', 4))(x)\n",
    "\n",
    "        # 5) GRU + atención\n",
    "        x = layers.Bidirectional(\n",
    "                layers.GRU(mp.get('gru_units', 128), return_sequences=True)\n",
    "            )(x)\n",
    "        attn = layers.Dense(1, activation='softmax')(x)   # softmax sobre time-axis\n",
    "        x = layers.Multiply()([x, attn])\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        # 6) Clasificador\n",
    "        x = layers.Dense(mp.get('dense_units', 128), activation='relu')(x)\n",
    "        x = layers.Dropout(mp.get('dropout', 0.5))(x)\n",
    "        out = layers.Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "        return tf.keras.Model(inputs=inp, outputs=out)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPV5uOmENlm6XMrZz4fS16d",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
